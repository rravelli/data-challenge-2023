{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On choisit XGBoost car il accepte des datasets avec des valeurs manquantes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données V1 - 2023/11/13\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratégie :  \n",
    "On remplace les heures par des floats, les identifiants de station par des entiers et on supprime la colonne 'date'.  \n",
    "(La suppression de la colonne 'way' a été oubliée)\n",
    "On a fait une division : 90% du dataset pour le train, 10% pour le test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/889A656D9A655928/Users/remir/Documents/EI3/STASC/DATA CHALLENGE/data-challenge-2023/2_model_exploration/train/train_xgboost.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/889A656D9A655928/Users/remir/Documents/EI3/STASC/DATA%20CHALLENGE/data-challenge-2023/2_model_exploration/train/train_xgboost.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./../data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/889A656D9A655928/Users/remir/Documents/EI3/STASC/DATA%20CHALLENGE/data-challenge-2023/2_model_exploration/train/train_xgboost.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m x_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/Xtrain_hgcGIrA.csv\u001b[39m\u001b[39m\"\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# features\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/889A656D9A655928/Users/remir/Documents/EI3/STASC/DATA%20CHALLENGE/data-challenge-2023/2_model_exploration/train/train_xgboost.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/889A656D9A655928/Users/remir/Documents/EI3/STASC/DATA%20CHALLENGE/data-challenge-2023/2_model_exploration/train/train_xgboost.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/Ytrain_yL5OjS4.csv\u001b[39m\u001b[39m\"\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m, usecols\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/889A656D9A655928/Users/remir/Documents/EI3/STASC/DATA%20CHALLENGE/data-challenge-2023/2_model_exploration/train/train_xgboost.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )  \u001b[39m# occupancy rate\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "path = \"./../../data\"\n",
    "x_data = pd.read_csv(path + \"/Xtrain_hgcGIrA.csv\", sep=\",\")  # features\n",
    "y_data = pd.read_csv(\n",
    "    path + \"/Ytrain_yL5OjS4.csv\", sep=\",\", usecols=[1]\n",
    ")  # occupancy rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mise en forme des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data.drop(\"date\", axis=1)  # on supprime la colonne date\n",
    "x_data[\"hour\"] = x_data[\"hour\"].apply(\n",
    "    lambda x: int(x[:2]) if isinstance(x, str) else np.nan\n",
    ")  # on transforme les strings des heures en float\n",
    "# on tranforme les identifiants de gare en entiers\n",
    "x_data[\"station\"] = x_data[\"station\"].astype(\"category\")\n",
    "cat_columns = x_data.select_dtypes([\"category\"]).columns\n",
    "x_data[cat_columns] = x_data[cat_columns].apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Création d'un dataset de test et de validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x_data.values, y_data.values, train_size=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoostREGRESSOR - 2023/11/13 - Préparation des données V1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise le regresseur et pas le classifieur car on ne cherche pas la classe de p0q0 mais sa valeur précise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import de XGBRegresseur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test sans jouer sur les paramètres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9872039534087059"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBR = XGBRegressor()\n",
    "XGBR.fit(X_train, y_train)\n",
    "XGBR.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score est assez bon, cependant, en jouant sur les paramètres, on va essayer de l'augmenter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choix de learning_rate et n_estimators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9896284435980697 {'learning_rate': 0.05, 'n_estimators': 2000}\n",
      "155.02244329452515 sec\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "\n",
    "my_kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "n_estimators = [300, 500, 1000, 2000]\n",
    "learning_rate = [0.005, 0.01, 0.05, 0.1]\n",
    "\n",
    "tuned_parameters = {\n",
    "    \"n_estimators\": n_estimators,\n",
    "    \"learning_rate\": learning_rate,\n",
    "}\n",
    "\n",
    "XGBR_params = GridSearchCV(\n",
    "    XGBRegressor(), tuned_parameters, cv=my_kfold, n_jobs=-1\n",
    ")\n",
    "\n",
    "XGBR_params.fit(X_train, y_train)\n",
    "print(XGBR_params.score(X_test, y_test), XGBR_params.best_params_)\n",
    "print(str(time() - start) + \" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Premier test avec\n",
    "\n",
    "- n_estimators = [300, 500, 1000, 2000]\n",
    "- learning_rate = [0.005, 0.01, 0.05, 0.1]\n",
    "\n",
    "On obtient :\n",
    "\n",
    "0.9896284435980697 {'learning_rate': 0.05, 'n_estimators': 2000}\n",
    "155.02244329452515 sec\n",
    "\n",
    "Le score obtenu est meilleur, mais on va essayer d'affiner ces résultats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deuxième test avec :\n",
    "\n",
    "- n_estimators = [1000, 1500, 2000, 2500, 3000]\n",
    "- learning_rate = [0.01, 0.025, 0.05, 0.075, 0.1]\n",
    "\n",
    "On obtient :\n",
    "\n",
    "0.9895109476080269 {'learning_rate': 0.025, 'n_estimators': 3000}  \n",
    "427.96634221076965 sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troisième test avec :\n",
    "\n",
    "- n_estimators = [2750, 3000, 3250, 3500]\n",
    "- learning_rate = [0.01, 0.025, 0.05, 0.075]\n",
    "\n",
    "On obtient :\n",
    "\n",
    "0.9895371733074723 {'learning_rate': 0.025, 'n_estimators': 3500}  \n",
    "564.882648229599 sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quatrième test avec :\n",
    "\n",
    "- n_estimators = [3000, 3250, 3500, 3750, 4000]\n",
    "- learning_rate = [0.01, 0.025, 0.05]\n",
    "\n",
    "On obtient :\n",
    "\n",
    "0.9895812224115239 {'learning_rate': 0.025, 'n_estimators': 4000}\n",
    "463.68704533576965 sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On choisit de s'arrêter là pour voir l'impact des autres paramètres sur le score. Les deux meilleurs couples sont, pour n_estimators et learning_rate (0.05,2000) et (0.025,4000). On utilisera le pre;ier pour optimiser les autres paramètres, puis, on verra quel couple permet d'avoir le meilleur score final. Dans l'éventuel cas où ce serait (0.025,4000), on pourra essayer d'augmenter encore n_estimators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- choix de max_depth et gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9892712110007462 {'max_depth': 7, 'min_child_weight': 8}\n",
      "456.5879855155945 sec\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "my_kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "max_depth = [5, 6, 7, 8, 9]\n",
    "min_child_weight = [5, 6, 7, 8, 9]\n",
    "\n",
    "tuned_parameters = {\n",
    "    \"max_depth\": max_depth,\n",
    "    \"min_child_weight\": min_child_weight,\n",
    "}\n",
    "\n",
    "XGBR_params = GridSearchCV(\n",
    "    XGBRegressor(n_estimators=2000, learning_rate=0.05),\n",
    "    tuned_parameters,\n",
    "    cv=my_kfold,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "XGBR_params.fit(X_train, y_train)\n",
    "print(XGBR_params.score(X_test, y_test), XGBR_params.best_params_)\n",
    "print(str(time() - start) + \" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Premier test avec\n",
    "\n",
    "- max_depth = [1, 10, 100]\n",
    "- min_child_weight = [1, 10, 100]\n",
    "\n",
    "On obtient :\n",
    "\n",
    "0.988740513845699 {'max_depth': 10, 'min_child_weight': 10}  \n",
    "442.56765842437744 sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deuxième test avec\n",
    "\n",
    "- max_depth = [5, 10, 25, 50]\n",
    "- min_child_weight = [5, 10, 25, 50]\n",
    "\n",
    "On obtient :\n",
    "\n",
    "0.989310996503391 {'max_depth': 5, 'min_child_weight': 5}  \n",
    "659.9102652072906 sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troisième test avec\n",
    "\n",
    "- max_depth = [3,5,7,10,15]\n",
    "- min_child_weight = [3,5,7,10,15]\n",
    "\n",
    "On obtient :\n",
    "\n",
    "0.9892469163908968 {'max_depth': 7, 'min_child_weight': 7}\n",
    "456.729660987854 sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quatrième test avec :\n",
    "\n",
    "- max_depth = [5,6,7,8,9]\n",
    "- min_child_weight = [5,6,7,8,9]\n",
    "\n",
    "On obtient :\n",
    "\n",
    "0.9892712110007462 {'max_depth': 7, 'min_child_weight': 8}  \n",
    "456.5879855155945 sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On choisit max_depth = 7 et min_child_weight = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- choix de reg_alpha et reg_lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9892045975248401 {'reg_alpha': 0.15, 'reg_lambda': 0.21}\n",
      "327.5475251674652 sec\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "my_kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "reg_alpha = [0.3, 0.14, 0.15, 0.16, 0.17]\n",
    "reg_lambda = [0.18, 0.19, 0.2, 0.21, 0.22]\n",
    "\n",
    "tuned_parameters = {\"reg_alpha\": reg_alpha, \"reg_lambda\": reg_lambda}\n",
    "\n",
    "XGBR_params = GridSearchCV(\n",
    "    XGBRegressor(\n",
    "        n_estimators=2000, learning_rate=0.05, max_depth=7, min_child_weight=8\n",
    "    ),\n",
    "    tuned_parameters,\n",
    "    cv=my_kfold,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "XGBR_params.fit(X_train, y_train)\n",
    "print(XGBR_params.score(X_test, y_test), XGBR_params.best_params_)\n",
    "print(str(time() - start) + \" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Premier test avec\n",
    "\n",
    "- reg_alpha = [0.1, 1, 10, 100]\n",
    "- reg_lambda = [0.1, 1, 10, 100]\n",
    "\n",
    "On obtient :\n",
    "\n",
    "0.9891920131398381 {'reg_alpha': 0.1, 'reg_lambda': 0.1}  \n",
    "143.50136017799377 sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deuxième test avec\n",
    "\n",
    "- reg_alpha = [0.05 , 0.1, 0.15, 0.2]\n",
    "- reg_lambda = [0.05 , 0.1, 0.15, 0.2]\n",
    "\n",
    "On obtient :\n",
    "\n",
    "0.9891699132652103 {'reg_alpha': 0.15, 'reg_lambda': 0.2}  \n",
    "241.633647441864 sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat est inférieur à partir du 10-millième.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troisième test avec\n",
    "\n",
    "- reg_alpha = [0.1, 0.125, 0.15, 0.175]\n",
    "- reg_lambda = [0.08 , 0.12, 0.16, 0.2, 0.25]\n",
    "\n",
    "On obtient :\n",
    "\n",
    "0.9891699132652103 {'reg_alpha': 0.15, 'reg_lambda': 0.2}\n",
    "296.16087889671326 sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On choisit de creuser autour de 0.15 et 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quatrième test avec\n",
    "\n",
    "- reg_alpha = [0.3, 0.14, 0.15, 0.16, 0.17]\n",
    "- reg_lambda = [0.18, 0.19, 0.2, 0.21, 0.22]\n",
    "\n",
    "On obtient :\n",
    "\n",
    "0.9892045975248401 {'reg_alpha': 0.15, 'reg_lambda': 0.21}  \n",
    "327.5475251674652 sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test finaux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9892045975248401"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBR1 = XGBRegressor(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    min_child_weight=8,\n",
    "    reg_alpha=0.15,\n",
    "    reg_lambda=0.21,\n",
    ")\n",
    "XGBR1.fit(X_train, y_train)\n",
    "XGBR1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9892679703760023"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBR2 = XGBRegressor(\n",
    "    n_estimators=4000,\n",
    "    learning_rate=0.025,\n",
    "    max_depth=7,\n",
    "    min_child_weight=8,\n",
    "    reg_alpha=0.15,\n",
    "    reg_lambda=0.21,\n",
    ")\n",
    "XGBR2.fit(X_train, y_train)\n",
    "XGBR2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient bien un score meilleur que pour les valeurs de paramètres par défaut.\n",
    "On explore un peu les résultats si on augment n_estimators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9892679703760023"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBR2_1 = XGBRegressor(\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.025,\n",
    "    max_depth=7,\n",
    "    min_child_weight=8,\n",
    "    reg_alpha=0.15,\n",
    "    reg_lambda=0.21,\n",
    ")\n",
    "XGBR2_1.fit(X_train, y_train)\n",
    "XGBR2_1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score n'évolue pas du tout, on choisit de garder XGBR2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBRegressor V2 - TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/#:~:text=In%20XGBoost%2C%20a%20hyperparameter%20is,tree%20depth%2C%20and%20regularization%20parameters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STASC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
